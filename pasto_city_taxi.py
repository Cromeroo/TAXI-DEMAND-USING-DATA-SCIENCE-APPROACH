# -*- coding: utf-8 -*-
"""Pasto_City_Taxi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hPwkTa8dN_U1VVyZJRx10oph5eeekPiP

<a id='top'><img src='https://media2.giphy.com/media/xUNd9H0CFkQEhKjkiI/giphy.gif?cid=ecf05e47jopikp4npjrz0jimbspe1dh1qhgfpjhuia94yxvu&rid=giphy.gif'></a>
<div class="list-group" id="list-tab" role="tablist">
  <h3 class="list-group-item list-group-item-action active" data-toggle="list"  role="tab" aria-controls="home">Table of Content</h3>
  <a class="list-group-item list-group-item-action" data-toggle="list" href="#intro" role="tab" aria-controls="profile">Introduction<span class="badge badge-primary badge-pill"></span></a></br>
  <h5><a class="list-group-item list-group-item-action" data-toggle="list" href="#data" role="tab" aria-controls="profile">Load data<span class="badge badge-primary badge-pill"></span></a></h5>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#exploration" role="tab" aria-controls="profile"> Data Exploration<span class="badge badge-secondary badge-pill"></span></a></dd>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#structure" role="tab" aria-controls="profile"> Establishing the data’s underlying structure.<span class="badge badge-secondary badge-pill"></span></a></dd>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#prepro" role="tab" aria-controls="profile"> Data preprocessing.<span class="badge badge-secondary badge-pill"></span></a></dd>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#mistakes" role="tab" aria-controls="profile"> Identifying mistakes and missing data.<span class="badge badge-secondary badge-pill"></span></a></dd><br/>
  <h5><a class="list-group-item list-group-item-action" data-toggle="list" href="#eda" role="tab" aria-controls="settings">Exploratory Data Analysis<span class="badge badge-primary badge-pill"></span></a></h5>
  <h5><a class="list-group-item list-group-item-action" data-toggle="list" href="#fe" role="tab" aria-controls="settings">Feature Engineering<span class="badge badge-primary badge-pill"></span></a></h5>
  <h5><a class="list-group-item list-group-item-action" data-toggle="list" href="#pred" role="tab" aria-controls="settings">Modeling and Prediction<span class="badge badge-primary badge-pill"></span></a></h5>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#clust" role="tab" aria-controls="profile"> Time series clustering for forecasting preparation.<span class="badge badge-secondary badge-pill"></span></a></dd>
  <dd><a class="list-group-item list-group-item-action" data-toggle="list" href="#prop" role="tab" aria-controls="profile"> Time Series Forecasting With Prophet.<span class="badge badge-secondary badge-pill"></span></a></dd>
"""

# Libraries
import numpy as np
import pandas as pd
# import pandasql as ps
import random
from random import sample
import seaborn as sns
from dtw import *
from os import listdir
from holidays_co import get_colombia_holidays_by_year as holiday
import matplotlib.pyplot as plt
from os.path import isfile, join
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import fbprophet
import calendar as cdr
# always make it pretty
plt.style.use('seaborn')

"""## <a id='intro'>Introduction

<p><strong>Business Context.</strong><br/>

<p><strong>Business Problem.</strong>

<p><strong>Analytical Context.</strong>

<h2 style="background-color:DodgerBlue; color:white" ><a id='data'>1. Load data</a></h2>
<a href='#top'><span class="label label-info">Go to the table of content</span></a>

- **File 1: “mastermind_solicitud.csv”**</br>
It offers detailed information about more than 2.4M taxicab service requests.<br/>
This table contains the most crucial information about the demand for taxicab services via requests. It identifies all requests made to the company, including request DateTime, address, service type, and client (if available). With this data,  demand behavior in the past can be analyzed and modeled to produce forecasts.
- **File 2: “mastermind_solicitudcancelada.csv”**</br>
This table contains useful information about the cancelation of requests. It identifies all cancelations made to the company, including some details like request identification and motive for cancellation. With this data, it could be possible to analyze the potential harmful factors that impact the client's perception of the company and ultimately affect the demand for their services, such as delays in the car's arrival or driver's unavailability to cover up the requested service.
- **File 3: “administracion_barrio.csv”**</br>
Contains neighborhood names for the city of Pasto.
- **File 4: “administración_direccion.csv”**</br>
This table contains all the addresses to which have requested a radio taxi service.
The information about the address and neighborhood of the request can be useful for geolocation analysis. It contains latitude and longitude information. However, this data is not reliable, so it is suggested not to use these columns.
- **File 5: “adminsitracion_directorio.csv”**</br>
This table contains information on the people who have requested a taxi service from the company.
This information can be useful to create profiles of users and link their requests.
- **File 6: “administracion_afiliacion.csv”**</br>
This table contains information on taxi drivers' affiliation with the company. It can be useful to check the number of taxi drivers affiliated with the company.
"""

# Create a list with the name of files
files_interesting = ['taxisrcp_mastermind_solicitud.csv', 'taxisrcp_mastermind_solicitudcancelada.csv',
                     'taxisrcp_administracion_barrio.csv', 'taxisrcp_administracion_direccion.csv',
                     'taxisrcp_administracion_directorio.csv', 'taxisrcp_administracion_afiliacion.csv']

dataFrames = {}

for file in files_interesting:
    # the name for the dataframe
    name_df = file.replace('taxisrcp_administracion_', '').replace('taxisrcp_mastermind_', '').replace('taxisrcp_parametrizacion_', '').replace('.csv', '')
    dataFrames[name_df] = pd.read_csv('Data/'+ file)

"""### <a id='exploration'>1.1 Data Exploration </a>

#### <a id='structure'>Establishing the data’s underlying structure. </a>
"""

measurer = np.vectorize(len)
def max_len(name_df):
    return measurer(dataFrames[name_df].values.astype(str)).max(axis=0)

dataFrames.keys()

"""#### 1. Solicitud"""

dataFrames['solicitud'].head(3)

dataFrames['solicitud'].shape

"""There are about more than 2.4M taxicab service requests."""

dataFrames['solicitud'].dtypes

list(zip(list(dataFrames['solicitud'].columns), list(max_len('solicitud'))))

"""Columns `creado` and `modificado` are DateTime, but in the data frame are type "object", these columns have 29 characters.

#### 2. Solicitud cancelada
"""

dataFrames['solicitudcancelada'].head(3)

dataFrames['solicitudcancelada'].shape

"""There are about more than 20K cancelation of requests."""

dataFrames['solicitudcancelada'].dtypes

list(zip(list(dataFrames['solicitudcancelada'].columns), list(max_len('solicitudcancelada'))))

"""The column `motivo cancelation` seems like to be a large text."""

dataFrames['solicitudcancelada'].motivo_cancelacion.unique

"""#### 3. Barrio"""

dataFrames['barrio'].head(3)

dataFrames['barrio'].shape

"""It seems there is 573 unique "barrio". However, we are going to explore in more detail."""

dataFrames['barrio'].dtypes

list(zip(list(dataFrames['barrio'].columns), list(max_len('barrio'))))

"""#### 4. Dirección"""

dataFrames['direccion'].head(3)

dataFrames['direccion'].shape

"""There are about more than 359K addresses."""

dataFrames['direccion'].dtypes

"""#### 5. Directorio"""

dataFrames['directorio'].head(3)

dataFrames['directorio'].shape

"""There are about more than 358K phone numbers and this is similar to the number of records in the table "direccion". Seems there is one address for each user (identified by the phone number)."""

dataFrames['directorio'].dtypes

list(zip(list(dataFrames['directorio'].columns), list(max_len('directorio'))))

"""We expect that the `numero` column's length will be ten because it is a phone number. We are going to explore in more detail later.

#### 6. Afiliacion
"""

dataFrames['afiliacion'].head(3)

dataFrames['afiliacion'].shape

"""It seems there are about 1.9K taxi drivers."""

dataFrames['afiliacion'].dtypes

"""#### <a id='mistakes'>Identifying mistakes and missing data. </a>
<a href='#top'><span class="label label-info">Go to the table of content</span></a>
"""

# This function allows us to see the percentage of missing values for each column.

def missing_data(df):
    percent_missing = df.isnull().sum() / len(df)
    missing_value_df = pd.DataFrame({'column_name': df.columns,
                                     'percent_missing': percent_missing})

    missing_value_df = missing_value_df.style.format({'percent_missing': "{:.2%}"})
    return missing_value_df

# Checking the missing values for "solicitud" table.
missing_data(dataFrames['solicitud'])

"""As we can see, `usuario_solicitante_id` has 98.7% missing values. We can handle missing values by dropping the column.<br/>
However, the `usuario_solicitante_id` referencing the operator of the company, who records the request.
"""

dataFrames['solicitud'].drop('usuario_solicitante_id', axis = 1, inplace = True)

# Checking the missing values for "solicitudcancelada" table.
missing_data(dataFrames['solicitudcancelada'])

"""This table does not have significant missing values. But we will remove records with "creado" missing."""

dataFrames['solicitudcancelada'].dropna(subset=['creado'], inplace=True)

# Checking the missing values for "barrio" table.
missing_data(dataFrames['barrio'])

"""The "barrio" dataset does not have missing data."""

# Checking the missing values for "direccion" table.
missing_data(dataFrames['direccion'])

"""The `direccion_completa`, `informacion_complementaria`, `barrio_id`, `usuario_id` and `barrio_texto` columns have 1.3%, 62.5%, 89.9%, 10.2% and 91.1% missind data, respectively.<br/>
There are a high number of missing values for the `barrio_id` column, so it is a challenge to have the address for each user, and do not count on the neighborhood associated.
"""

# Checking the missing values for "directorio" table.
missing_data(dataFrames['directorio'])

"""The variable `empresa_id` is not essential. This variable is directly related to the company to which the vehicle belongs.<br/>
The variable `perfil_id` is a foreign key to the administracion_perfil table witch contains the birth date and gender of users, so it is a pity that we do not have this information.<br/>
The variable `usuario_id` referencing the operator of the company, who records the request.<br/>
We will remove these variables.
"""

dataFrames['directorio'].drop(['modificado', 'empresa_id', 'perfil_id', 'usuario_id'], axis = 1, inplace = True)

# Checking the missing values for "afiliacion" table.
missing_data(dataFrames['afiliacion'])

"""We expect that the difference between "creado" and "modificado" do not exceed one day or even some minutes."""

dataFrames['solicitud']['creado'] = pd.to_datetime(dataFrames['solicitud']['creado'])
dataFrames['solicitud']['modificado'] = pd.to_datetime(dataFrames['solicitud']['modificado'])

dif_dates = dataFrames['solicitud']['modificado'] - dataFrames['solicitud']['creado']

dif_dates.describe()

threshold = pd._libs.tslibs.timedeltas.Timedelta('1 days 00:00:00.000000')
index_dif_intrtg = [index for index, value in enumerate(dif_dates) if value >= threshold]

dataFrames['solicitud'].iloc[index_dif_intrtg,:].tail()

len(index_dif_intrtg)/len(dif_dates)

"""We expect that difference between these columns will be less than one day, but there are 1.62% of requests (2.4M) even though this apparent inconsistency is not material.

#### <a id='prepro'>Data preprocessing. </a>

We are going to pre-process the data to transform it into a format amenable to later analysis.

If we observed the above datasets, the `latitud` and `longitud` columns we can not use them.  So, we are going to remove it.
The `creado` and `modificado` columns have both the information of date and time in it, so we will convert to "DateTime" type and extract some interesting columns like the month, week, weekday, etc.
"""

# Drop the "latitud" and "longitud" as it is of no use.
# dataFrames['direccion'].drop(['latitud', 'longitud'], axis = 1, inplace = True)

# This function allows us to extract the month, week, weekday, etc.

def date_time(df, column):

    """
    Creates time series features from datetime index.
    """
    df[column] = pd.to_datetime(df[column])
    df['date'] = df[column].dt.date
    df['year_month'] = df[column].dt.to_period('M').astype(str)
    df['year'] = df[column].dt.year
    df['quarter'] = df[column].dt.quarter
    df['month'] = df[column].dt.month
    df['day'] = df[column].dt.day
    df['hour'] = df[column].dt.hour
    df['dayofyear'] = df[column].dt.dayofyear
    df['weekday'] = df[column].dt.weekday_name
    df['dayofweek'] = df[column].dt.dayofweek
    df['week'] = df[column].dt.week
    df['weekofyear'] = df[column].dt.weekofyear

# Calculate DateTime columns to the "solicitudcancelada" table.
date_time(dataFrames['solicitudcancelada'], 'creado')

# Calculate DateTime columns to the "directorio" table.
date_time(dataFrames['directorio'], 'creado')

"""Dirección"""

sol_dir.columns

len(sol_dir.direccion_id.unique())

dire = pd.DataFrame({'id': sol_dir.direccion_id.unique()})

dire.head()

address_df = pd.merge(pd.DataFrame({'id': sol_dir.direccion_id.unique()}), dataFrames['direccion'][['id', 'direccion_completa']])

address_df.shape

len(address_df.direccion_completa.unique())

"""Duplicate address records"""

duplicateRowsAddress = address_df[address_df.duplicated('direccion_completa')]
duplicateRowsAddress.shape

duplicateRowsAddress

address_df[address_df.direccion_completa == 'Condominio altos de la colina']

"""Address data frame unique"""

address_df_unique = address_df.drop_duplicates(subset='direccion_completa', keep='first', inplace=False)

address_df_unique.shape

address_df_unique.direccion_completa.describe()

address_df_unique.to_csv('Data/address.csv', index=False)

address_df.head()

"""<h3>Now we have inspected the structure, identified mistakes and missing data. Let's Explore!</h3>

<h2 style="background-color:DodgerBlue; color:white" ><a id='eda'>2. Exploratory Data Analysis</a></h2>
<a href='#top'><span class="label label-info">Go to the table of content</span></a>

We will utilize simple plots to explore relationships between some variables and the volume of requests and identify any taxicab usage patterns and some crucial days.

First, we are going to start with some univariate distribution to understand the data better.
"""

pd.DataFrame(dataFrames['solicitud'].groupby('tipo_servicio')['id'].count().rename('Percentage')
             /len(dataFrames['solicitud'])).style.format({'Percentage': "{:.2%}"})

"""It is evident that `SC`, which we know from the company that states for "Servicio Corriente" (ordinary service), takes most of the requests made to the company.

The following are the top 10 most frequent reasons for canceling a request.
"""

pd.DataFrame(dataFrames['solicitudcancelada'].groupby('motivo_cancelacion')['id'].count().rename('Percentage')
             /len(dataFrames['solicitudcancelada'])).sort_values('Percentage', ascending=False).head(10).style.format({'Percentage': "{:.2%}"})

"""From this table, it is evident there is a problem in the supply causing many cancellations. Reason 1 “No vehicles were found to attend the request” account for more than 60% of the cases, so it seems the primary reason for cancelations is due to the company. Reason 2, “I do not need the service anymore” accounts for almost 30%. However, this reason can be due to the client getting another service."""

pd.DataFrame(dataFrames['barrio'].groupby('barrio')['id'].count().rename('Count')).sort_values('Count', ascending=False).head(10)

pd.DataFrame(dataFrames['barrio'].groupby('barrio')['id'].count().rename('Count')).describe()

"""The distribution of neighborhoods is like one would expect, each neighborhood to appear just once. However, there are several of them that appear two or three times."""

pd.DataFrame(dataFrames['direccion'].groupby('direccion_completa')['id'].count().rename('Count')).sort_values('Count', ascending=False).head(10)

pd.DataFrame(dataFrames['direccion'].groupby('direccion_completa')['id'].count().rename('Count')).describe()

"""Several addresses are repeated when one would expect that each address is found only once. Furthermore, the most common address is an invalid one.

#### Directorio
"""

pd.DataFrame(dataFrames['directorio'].groupby('numero')['id'].count().rename('Count')).sort_values('Count', ascending=False).head(10)

pd.DataFrame(dataFrames['directorio'].groupby('numero')['id'].count().rename('Count')).describe()

"""Several phone numbers are repeated when one would expect that each phone number is found only once, even though most phone numbers appear only once."""

dataFrames['directorio'].shape

len(dataFrames['directorio'].numero.unique())

"""There are 358K users, but the records of phone numbers unique are 251K."""

dataFrames['directorio'].numero.unique()

len_numero = dataFrames['directorio']['numero'].str.len()

len_numero.describe([0.01, 0.05, 0.1, 0.15])

"""We expect that phone numbers have a length of 7 or 10 for landline phones or mobile phones. There are a few records that do not accomplish this statement.<br/>
We will inspect the `numero` variable in more detail to identify if it has characters and why duplicity in records there are.
"""

# Flag "number" if it contains characters non-digits, except  numbers like 3.4589e+9
dataFrames['directorio']['flag_char_num'] = dataFrames['directorio']['numero'].apply(lambda x: bool(re.search(r"\D", str(x)))
                                                                                     & ~bool(re.search('\.0$', str(x))))

# Get only digits of the "number" string.
dataFrames['directorio']['phone_number'] = dataFrames['directorio']['numero'].apply(lambda x: re.sub(r"\D", "", str(x))
                                                                                    if not bool(re.search('\.0$', str(x)))
                                                                                    else int(float(str(x))))

# try:
#     dataFrames['directorio']['phone_number'] = dataFrames['directorio']['phone_number'].apply(lambda x: int(float(x)))
# except:
#     dataFrames['directorio']['phone_number'] = None

dataFrames['directorio']['phone_number'] = dataFrames['directorio']['phone_number'].apply(lambda x: int(x)
                                                                                          if x
                                                                                          else x)

dataFrames['directorio'][dataFrames['directorio'].flag_char_num & ~dataFrames['directorio'].numero.isna()]

"""We can see that only 105 records have an inconsistent phone number, so we will extract the phone number and check the requests' behavior for the non-valid phone numbers separately."""

# Get the phone number's length.
dataFrames['directorio']['len_number'] = dataFrames['directorio']['phone_number'].astype(str).str.len()

dataFrames['directorio'].groupby('len_number').size()

"""Most users in the directory have a mobile, following by the users with a landline."""

# Check duplicated records by "numero"
rowsDirectorio_itng = dataFrames['directorio'][(dataFrames['directorio'].len_number > 0) & ~(dataFrames['directorio'].flag_char_num)]
duplicateRowsDirectorio = rowsDirectorio_itng[rowsDirectorio_itng.duplicated('phone_number')]
duplicateRowsDirectorio.shape

dataFrames['directorio'][dataFrames['directorio'].phone_number == 988935951]

len(duplicateRowsDirectorio.phone_number.unique())

"""We can see that there are 45,188 records with a phone_number duplicated. Usually, the `creado` and `direccion_id` columns are different for the duplicated records.<br/>
However, the `id` is unique, so we will join `directorio` table with `soliciutud` table and check more inconsistencies as we advance.

#### Understand the key variables of interest

<li>Solicitud</li>
"""

solicitud_df = dataFrames['solicitud'].drop(['modificado', 'status_id', 'usuario_id', 'tipo_servicio'], axis = 1).copy()

# Calculate DateTime columns to the "solicitud" table.
date_time(solicitud_df, 'creado')

solicitud_df.head()

# solicitud_df.to_csv('Data/solicitud.csv', index=False)

monthly_df = solicitud_df.groupby('year_month')['id'].count()
plt.figure(figsize=(10,5))
plt.plot(monthly_df)
plt.title('Number of requests per year-month')
plt.xlabel('date')
plt.ylabel('number of requests')
plt.xticks(rotation=90)
plt.show()

"""The variable `creado` spans the range from 06-Feb-2017 to 19-aug-2020. That is around 3.5 years of data.
We can observe a seasonal behavior, such that for every year, February is the month with the least requests and grows steadily to December, after which falls to next February. That is true for 2017-2019 but is different for 2020, where requests kept falling beyond February to a low in April. This last fact is explained by the bio-security measures taken by the government starting March/2020.
"""

weekdays_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
weekly_df = solicitud_df.groupby('weekday')['weekday'].count().sort_values()
weekly_df = weekly_df[weekdays_order]
plt.figure(figsize=(10,5))
plt.plot(weekly_df)
plt.title('Number of requests per weekday')
plt.xlabel('hour')
plt.ylabel('number of requests')
plt.xticks(rotation=90)
plt.show()

"""We can observe a seasonal behavior, such that Mondays have the least requests, following a growth all the way to Saturday, after which falls sharply to next Monday. We may conclude that Fridays and Saturdays have the most demand for taxicabs, way above other days of the week."""

hourly_df = solicitud_df.groupby('hour')['hour'].count()
plt.figure(figsize=(10,5))
plt.plot(hourly_df)
plt.title('Number of requests per hour')
plt.xlabel('hour')
plt.ylabel('number of requests')
plt.xticks(hourly_df.index, rotation=90)
plt.show()

"""We observe a highly seasonal behavior. This behavior is explained by factors such as rush hours between 6-8 am and 6-9 pm, and lunchtime between 1-2 pm. It can also be noticed that the hardest rush hour is the evening one. However, this can be biased by Fridays and Saturdays."""

weekly_hourly_df = solicitud_df.groupby(['weekday','hour','day'])['id'].nunique().reset_index()
plt.figure(figsize=(20,5))
sns.boxplot(x='hour',\
            y='id',\
            hue='weekday',
            data=weekly_hourly_df)
plt.title('Average number of requests per hour per weekday')
plt.xlabel('hour')
plt.ylabel('number of requests')
plt.show()

"""This is very interesting. In the early morning hours, there are two days that behave differently from the rest, which is Saturday and Sundays. This can be explained due to the fact that these days are the non-working days for most people. However, for the rest of the day, they seem to have lower demand than working days. A similar trend is observed for late-night hours, where the two different days are Friday and Saturday, the party days."""

weekly_hourly_df = solicitud_df.groupby(['weekday','hour','day'])['id'].count().reset_index()
weekly_hourly_df = weekly_hourly_df.groupby(['weekday','hour'])['id'].median().reset_index()
weekly_hourly_df = weekly_hourly_df.pivot(index='hour', columns='weekday', values='id')
weekly_hourly_df = weekly_hourly_df[weekdays_order]
plt.figure(figsize=(10,5))
plt.plot(weekly_hourly_df)
plt.title('Median number of requests per hour per weekday')
plt.xlabel('hour')
plt.ylabel('number of requests')
plt.legend(weekly_hourly_df.columns)
plt.show()

"""This graph shows a similar tendency for almost every day, except for the weekends."""

grp_year_month = solicitud_df.groupby(['year', 'month'])['id'].count()
grp_year_month_norm = grp_year_month/[cdr.monthrange(year, month)[1] for (year, month) in grp_year_month.index]

plt.figure(figsize=(10,5))
grp_year_month_norm.unstack('year').plot()
plt.title('Average of requests per day in the respective month')
plt.xlabel('month')
plt.ylabel('average number of requests')
plt.xticks(list(range(1,13)), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']);

"""We can see that the number of day-to-day requests on average is less for February, even though we do not have info in January for the year 2017, and 2020 is atypical due to the pandemic.
Years 2018 and 2019 have similar behavior. In contrast, 2017 shows an average of requests significantly less, particularly for the first semester of this year.
The year 2020 started well, but due to the pandemic, the year is atypical.
Maybe it would be convenient to take on count the information since July 2017 for the model.
"""

grp = solicitud_df.groupby(['year', 'weekofyear', 'weekday', 'hour'], as_index=False)['id'].count()

sns.pairplot(grp.dropna(),
             hue='hour',
             palette='Paired',
             x_vars=['hour','weekday',
                     'year','weekofyear'],
             y_vars='id',
             height=5,
             plot_kws={'alpha':0.15, 'linewidth':0}
            )
plt.suptitle('Taxicab Requests by Hour, Day of Week, Year and Week of Year')
plt.show()

grp_year_month = dataFrames['solicitudcancelada'].groupby(['year', 'month'])['id'].count()
grp_year_month_norm = grp_year_month/[cdr.monthrange(year, month)[1] for (year, month) in grp_year_month.index]

plt.figure(figsize=(10,5))
grp_year_month_norm.unstack('year').plot()
plt.title('Average of requests per day in the respective month')
plt.xlabel('month')
plt.ylabel('average number of requests')
plt.xticks(list(range(1,13)), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']);

"""We have info on cancelations since Oct 2018, and we can see that the higher number of cancelations happens in December.
In 2019 we observed that the number of cancelations is between 20 and 40  day-to-day requests on average.

<h2 style="background-color:DodgerBlue; color:white" ><a id='fe'>3. Feature Engineering</a></h2>
<a href='#top'><span class="label label-info">Go to the table of content</span></a>

##### Holidays

We expect that some holidays, no-car days, and other days like “El Carnaval de Negros y Blancos,” have a behavioral different from the other days, maybe a higher number of requests. We collected data from the “holidays_co” a package in python to get the holidays in Colombia to validate this hypothesis. We collected the ‘no-car days’ from [Alcaldia municipal de pasto.](https://www.pasto.gov.co.)
"""

solicitud_df.groupby('year').size()

holidays = pd.concat([pd.DataFrame(holiday(2017)), pd.DataFrame(holiday(2018)),
                      pd.DataFrame(holiday(2019)), pd.DataFrame(holiday(2020))])

"""[Día sin carro y sin moto.](https://www.pasto.gov.co/index.php/buscar-articulos-o-documentos?searchword=dia%20sin%20carro&ordering=oldest&searchphrase=all&limit=20)<br/>
Día del taxista: 7 de Mayo<br/>
[Carnaval de Negros y Blancos.](https://es.wikipedia.org/wiki/Carnaval_de_Negros_y_Blancos)<br/>
"""

date = ['2017-09-21', '2017-12-28', '2018-09-26', '2018-12-28', '2019-06-05', '2019-09-25', '2019-12-28',
        '2017-05-07', '2018-05-07', '2019-05-07', '2020-05-07',
        '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07',
        '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05', '2018-01-06', '2018-01-07',
        '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06', '2019-01-07',
        '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07',]
celebration = ['Día sin carro']*7
celebration.extend(['Día taxista']*4)
celebration.extend(['Carnaval']*24)
other_celebr = pd.DataFrame({'date': date,
                             'celebration': celebration})

total_celebr = pd.concat([holidays, other_celebr]).reset_index()
total_celebr['date'] = pd.to_datetime(total_celebr['date'])
total_celebr = total_celebr.set_index('date')

total_celebr.index.is_unique

total_celebr.shape

# total_celebr.loc[~total_celebr.index.duplicated()].drop(columns='index').to_csv('Data/holidays.csv')

# dataFrames['solicitud']['date'] = pd.to_datetime(dataFrames['solicitud']['creado'].apply(lambda x: x.date()))

grp = solicitud_df.groupby('date')
agg1 = grp.id.count().rename('Count', inplace=True)

agg1 = pd.merge(agg1, total_celebr.loc[~total_celebr.index.duplicated()], how='left', left_index=True, right_index=True)

agg1['Marca_celebr'] =  np.where(agg1['celebration'].isnull(), False, True)
# agg1['Marca_celebr'] =  agg1['celebration'].apply(lambda x: np.where(np.isnan(x), False, True))

agg1[320:340]

agg1['fecha'] = agg1.index

agg1.head()

agg2 = pd.DataFrame(agg1.groupby('celebration')['Count'].sum().rename('N Solicitudes'))

agg3 = agg1.groupby(['celebration'])['Count'].describe().sort_values(by=['50%'])

agg3

celebr_descr = pd.concat([agg2, agg3['count'].rename('N días'), agg3[['mean', '50%']] ], axis=1, sort=False).sort_values(by=['50%'])
# celebr_descr.style.format({'mean': "{:.2f}"}, inplace=True)
celebr_descr

celebr_descr[['N días','mean', '50%']].rename(columns={'mean':'Mean', '50%':'Median'}).style.background_gradient(cmap='Blues', subset=['Median'])

"""We can see that days on the Carnaval, día de los reyes magos, día de la inmaculada concepción y el día sin carro have a higher number of requests on
average and median by day, and the other days have a lower number of request, specially “Viernes y Jueves Santo”.
"""

date_time(agg1, 'fecha')

no_celebr = agg1[agg1['celebration'].isnull()]

no_celebr.groupby(['year', 'weekday'])['Count'].describe().sort_values(by=['year', '50%'])

def introduce_nulls(df):
    idx = pd.date_range(df.fecha.dt.date.min(), df.fecha.dt.date.max())
    #df = df.set_index('fecha')
    df = df.reindex(idx)
    df.reset_index(inplace=True)
    df.rename(columns={'index': 'date'}, inplace=True)
    return df

def calmap(df, title, scale):
    years = df.year.unique().tolist()
    fig = make_subplots(rows=len(years), cols=1, shared_xaxes=True, vertical_spacing=0.005)
    r = 1
    for year in years:
        data = df[df['year'] == year]
        data = introduce_nulls(data)
        fig.add_trace(go.Heatmap(
            z=data.Count,
            x=data.week,
            y=data.weekday,
            hovertext=data.fecha.dt.date,
            coloraxis="coloraxis",
            name=year,),
            r, 1)
        fig.update_yaxes(title_text=year, tickfont=dict(size=5), row=r, col=1)
        r += 1
    fig.update_xaxes(range=[1, 53], tickfont=dict(size=10), nticks=53)
    fig.update_layout(coloraxis={'colorscale': scale})
    fig.update_layout(template='seaborn', title=title)
    return fig

"""To validate that the days mentioned above are different from days common, the following graph shows  the number of requests by each year and weekday along the weeks, without the days mentioned above."""

calmap(no_celebr, 't', 'YlGnBu')

"""We can observe that Friday and Saturday have a greater number of requests for almost all the weeks  each year, except 2020. There are some days in black (lower number of requests) and we should  investigate the reasons why.
It is evident the low number of requests for weeks 12 and 19.due to the pandemic in 2020.
Finally, days like Friday and Saturday and the higher request days mentioned above have different  behavior from the other days.

----------------------------------------------------------------------------------------------------------------------------------------------------

We will calculate some variables at the client's level, with the goal to characterize clients and get differential relationships between client's clusters and the behavior of their taxicabs requests.

- Phone Type
"""

dataFrames['directorio']['check_phone'] = np.where(dataFrames['directorio']['len_number'] == 10, 'mobile',
                                                   np.where(dataFrames['directorio']['len_number'] == 7, 'landline',
                                                            np.where(dataFrames['directorio']['len_number'] == 4, 'len_4',
                                                                     np.where(dataFrames['directorio']['len_number'] > 0, 'other', 'missing'))))

dataFrames['directorio'].head()

# dataFrames['directorio'][['id', 'numero', 'direccion_id', 'date', 'year', 'month', 'flag_char_num',
#                           'phone_number', 'len_number', 'check_phone']].to_csv('Data/directorio.csv', index=False)

solicitud_df = pd.read_csv('Data/solicitud.csv')
directorio_df = pd.read_csv('Data/directorio.csv')

"""Merge `solicitud` table with `directorio` to identify the user's number of requests. The user can identify with the phone number."""

# sol_dir = pd.merge(solicitud_df,
#                    dataFrames['directorio'][['id', 'numero', 'direccion_id', 'year', 'month', 'flag_char_num',
#                                              'phone_number', 'len_number', 'check_phone']],
#                    how='left', left_on='directorio_id', right_on='id')

sol_dir = pd.merge(solicitud_df, directorio_df,
                   how='left', left_on='directorio_id', right_on='id')

solicitud_df.shape

sol_dir.shape

sol_dir.id_y.count()/sol_dir.shape[0]

"""99% de la base cruza"""

sol_dir.len_number.count()

sol_dir.groupby('len_number').size()/sum(sol_dir.groupby('len_number').size())

"""It seems the join does not have duplicate records."""

fig, axs = plt.subplots(2,2, figsize=(18, 10), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = .3, wspace=.2)

axs = axs.ravel()
grp_year_month = sol_dir.groupby(['len_number', 'year_x', 'month_x'])['id_x'].count()
grp_year_month_norm = grp_year_month/[cdr.monthrange(year, month)[1] for (_, year, month) in grp_year_month.index]

for (i, year) in enumerate(sol_dir.year_x.unique()):
#     plt.subplot(2,2,i+1)
#     axs[i].plot(np.array(grp_year_month_norm[grp_year_month_norm.index.get_level_values(1) == year].unstack('len_number')))
    grp_year_month_norm[grp_year_month_norm.index.get_level_values(1) == year].unstack('len_number').plot(ax = axs[i], color=['black', 'tan', 'springgreen', 'peru', 'tab:pink',
                                                                                                                              'green', 'lime', 'tab:purple', 'silver',
                                                                                                                              'gold', 'tab:cyan'])
    axs[i].legend(title='length phone number', loc='upper right')
    axs[i].set_title('Average of requests per day in the respective month:' + str(year))
    axs[i].set_xlabel('month')
    axs[i].set_ylabel('average number of requests');
    axs[i].set_xticks(list(range(1,13)), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']);

"""We can see that mobile (phone number length  10) has a higher request on average per day, and the requests from landline (length phone number 7) have around 200-400 requests on average per day.<br/>
There are many requests from phone numbers with four digits, even though we do not know length four what means. <br/>
On the other hand, requests from numbers with different sizes seem to be an inconsistency of the data, so we will remove them.
"""

sol_dir.groupby('check_phone').size()

sol_dir.groupby('check_phone').size()/sol_dir.groupby('check_phone').size().sum()

sol_dir[sol_dir.check_phone == 'missing'].numero.describe()

sol_dir[sol_dir.check_phone == 'mobile'].phone_number.astype(np.int64).describe([0.9, 0.95, 0.99])

sol_dir[sol_dir.check_phone == 'landline'].phone_number.astype(int).describe([0.01, 0.05, 0.1, 0.9, 0.95, 0.99])

sol_dir.groupby('check_phone').size()/(sol_dir.groupby('check_phone').size().sum())

"""-----------------------------------------------------------------------------------------------------------------------------------------

- Month old
"""

# This function returns the month difference between two dates.
def diff_month(row):
    return(row['year_x'] - row['year_y'])*12 + row['month_x'] - row['month_y']

sol_dir['months_old'] = sol_dir.apply(diff_month, axis=1)

# sol_dir.to_csv('Data/sol_dir.csv', index=False)

sol_dir = pd.read_csv('Data/sol_dir.csv')

grp = sol_dir.groupby(['check_phone', 'phone_number']).agg({'id_x': 'count', 'months_old':['min', 'max']})

sol_dir.check_phone.unique()

fig, axs = plt.subplots(2,2, figsize=(18, 10), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = .3, wspace=.2)

axs = axs.ravel()
grp = sol_dir.groupby(['check_phone', 'phone_number']).agg({'id_x': 'count', 'months_old': 'max'})
grp.columns = ['number of requests', 'months_old']

for (i, check_phone) in enumerate(['mobile', 'landline', 'len_4', 'other']):
    grp[grp.index.get_level_values(0) == check_phone].plot(ax = axs[i], kind='scatter', x='months_old', y='number of requests')

    axs[i].set_title('Number of requests:' + str(check_phone))
    axs[i].set_ylabel('Number of requests');

"""We expect that clients with high `months_old` have more requests, and we can see that, but the relationship seems week. Additionally, we can see a few clients with extreme values of requests."""

fig, axs = plt.subplots(2,2, figsize=(18, 10), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = .3, wspace=.2)

axs = axs.ravel()
grp['log_month'] = np.log(grp['months_old'] + 1)
grp['log_count'] = np.log(grp['number of requests'])

for (i, check_phone) in enumerate(['mobile', 'landline', 'len_4', 'other']):
    grp[grp.index.get_level_values(0) == check_phone].plot(ax = axs[i], kind='scatter', x='log_month',
                                                           y= 'log_count')

    axs[i].set_title('Log Number of requests:' + str(check_phone))
    axs[i].set_ylabel('Log Number of requests');

"""If we plot the logarithm for both variables, we can see that the relationship seems strong, and the extreme values seem to disappear.<br/>
We will calculate the correlation between months_old and the number of taxicab's requests.
"""

corrmat = grp.corr()
mask = np.tril(grp.corr())
plt.figsize=(12,12)
ax = sns.heatmap(corrmat, annot=True, mask=mask) #notation: "annot" not "annote"
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5);

"""We can see that the correlation is higher when we use the logarithm of the variables."""

for (i, check_phone) in enumerate(['mobile', 'landline', 'len_4', 'other']):
    grp2 = grp[grp.index.get_level_values(0) == check_phone]
    corrmat = grp2.corr()
    print('type phone: {}, corr: {:.2%}'.format(check_phone, corrmat['months_old'][0:1][0]))

for (i, check_phone) in enumerate(['mobile', 'landline', 'len_4', 'other']):
    grp2 = grp[grp.index.get_level_values(0) == check_phone]
    corrmat = grp2.corr()
    print('type phone: {}, corr: {:.2%}'.format(check_phone, corrmat['log_month']['log_count']))

"""-----------------------------------------------------------------------------------------------------------------------------------------

- slope
"""

sol_dir.columns

group = sol_dir.groupby(['phone_number', 'date'], as_index=False).agg({'id_x': 'count'})
group_m = sol_dir.groupby(['phone_number', 'year_month'], as_index=False).agg({'id_x': 'count'})
group_q = sol_dir.groupby(['phone_number', 'year_x', 'quarter'], as_index=False).agg({'id_x': 'count'})

def slope(grp):
    xs = np.array(range(1,len(grp)+1))
    ys = grp.id_x
    grp['mean_rq'] = ys.mean()
    grp['n_rq'] = ys.sum()
    grp['min_rq'] = ys.min()
    grp['max_rq'] = ys.max()
    grp['std_rq'] = ys.std()
    grp['slope'] = (xs.mean()*ys.mean() - (xs*ys).mean()) / (xs.mean()**2 - (xs*xs).mean())
    return grp

daily_slope = group.groupby('phone_number').apply(slope)
monthly_slope = group_m.groupby('phone_number').apply(slope)
quarterly_slope = group_q.groupby('phone_number').apply(slope)

daily_slope.describe()

monthly_slope.describe()

quarterly_slope.describe()

daily_slope[np.isnan(daily_slope.slope)]

"""NaN slope means that the customer has requested a taxicab only once."""

daily_slope.tail()

monthly_slope.head()

quarterly_slope.head()

quarterly_slope_ = quarterly_slope.groupby('phone_number', as_index=False).agg({'n_rq':'min', 'mean_rq':'min', 'min_rq':'min',
                                                                                'max_rq':'min', 'std_rq':'min', 'slope':'min'})

quarterly_slope_.rename(columns={'mean_rq':'mean_rq_q', 'min_rq':'min_rq_q', 'max_rq':'max_rq_q', 'std_rq':'std_rq_q',
                                 'slope':'slope_q'}, inplace=True)

monthly_slope_ = monthly_slope.groupby('phone_number', as_index=False).agg({'mean_rq':'min', 'min_rq':'min', 'max_rq':'min',
                                                                            'std_rq':'min', 'slope':'min'})

monthly_slope_.rename(columns={'mean_rq':'mean_rq_m', 'min_rq':'min_rq_m', 'max_rq':'max_rq_m', 'std_rq':'std_rq_m',
                               'slope':'slope_m'}, inplace=True)

daily_slope_ = daily_slope.groupby('phone_number', as_index=False).agg({'mean_rq':'min', 'min_rq':'min', 'max_rq':'min',
                                                                        'std_rq':'min', 'slope':'min'})

daily_slope_.rename(columns={'mean_rq':'mean_rq_d', 'min_rq':'min_rq_d', 'max_rq':'max_rq_d', 'std_rq':'std_rq_d',
                             'slope':'slope_d'}, inplace=True)

sol_dir_grp = sol_dir.groupby('phone_number', as_index=False)['months_old'].max()
customer_df = pd.merge(pd.merge(pd.merge(pd.merge(quarterly_slope_, monthly_slope_), daily_slope_),
                                sol_dir[['phone_number', 'len_number', 'check_phone']].drop_duplicates()),
                                sol_dir_grp)

# customer_df.to_csv('Data/customer_df.csv', index=False)

daily_slope_.shape

customer = pd.read_csv('Data/customer_df.csv')

customer[customer.phone_number == 5]

customer_dummies = pd.get_dummies(customer)

customer_dummies['n_rq_log'] = np.log(customer_dummies['n_rq'] + 1)
customer_dummies['flag_once_rq'] = np.where(np.isnan(customer_dummies['slope_d']), 1, 0)

customer_dummies['flag_once_rq'].describe()

"""45% of the customers has requested a taxicab only once."""

corrmat = customer_dummies.corr()

corrmat['n_rq'].sort_values()

import math
var_intrg = customer_dummies.columns.difference(['phone_number', 'check_phone', 'len_number'])
customer_log = customer_dummies[var_intrg].apply(lambda x : x+1)
customer_log = pd.concat([customer_log.apply(np.log, axis=1), customer_dummies['phone_number']], axis=1)

corrmat_log = customer_log.corr()
corrmat_log['n_rq'].sort_values()

pd.merge(pd.merge(pd.DataFrame({'corr_raw_var': corrmat['n_rq'].sort_values()}),
                  pd.DataFrame({'corr_log_nq': corrmat['n_rq_log'].sort_values()}),
                  left_index=True, right_index=True),
         pd.DataFrame({'corr_log_var': corrmat_log['n_rq'].sort_values()}),
         left_index=True, right_index=True).style.background_gradient(cmap='Blues', axis=1)

"""We will use the raw variables `slope_q` and `slope_m` because they are not benefited from calculating the logarithm.<br/>
Additionally we will use the flag variables `check_phone` and `flag_once_rq` as raw variables.<br/>
The variables `mean_rq_q`, `mean_rq_m`, `max_rq_q`, `max_rq_m`, `std_rq_q`, `std_rq_m` and `months_old` will be used as logarithm.
"""

customer_dummies.columns

raw_vars = ['phone_number', 'slope_q', 'slope_m', 'check_phone_landline', 'check_phone_len_4', 'check_phone_missing',
            'check_phone_mobile', 'check_phone_other', 'flag_once_rq', 'n_rq']
log_vars = ['phone_number', 'mean_rq_q', 'mean_rq_m', 'max_rq_q', 'max_rq_m', 'std_rq_q', 'std_rq_m', 'months_old']

customer_vars = pd.merge(pd.merge(customer_dummies[raw_vars], customer_log[log_vars]), customer[['phone_number', 'check_phone']])
customer_vars.shape

duplicateRowscustomer_vars = customer_vars[customer_vars.duplicated('phone_number')]

duplicateRowscustomer_vars

customer_dummies[customer_dummies.phone_number == 5]

"""-----------------------------------------------------------------------------------------------------------------------------------------

# <a id='pred'>4. Modeling and Prediction</a>
<a href='#top'><span class="label label-info">Go to the table of content</span></a>

#### <a id='clust'>Time series clustering for forecasting preparation </a>

The purpose is group items with a similar underlying structure before diving into modeling using several clustering approaches. This way, we could similarly pre-process series within groups and train a model for each cluster, which specializes in learning this underlying structure.

We will use a naive hierarchical method to cluster the series, point out the shortcoming of this approach, and discuss a method to approach these shortcomings called Dynamic Time Warping.
"""

sol_dir = pd.read_csv('Data/sol_dir.csv')

group = sol_dir.groupby(['phone_number', 'date_x'], as_index=False).agg({'id_x': 'count'})

group_pivot = group.pivot(index='phone_number', columns='date_x')

group_pivot.fillna(0, inplace=True)

group_pivot.head()

# columns = ['d' + str(i) for i in range(1, len(group_pivot.columns)+1)]
columns = [i for i in range(1, len(group_pivot.columns)+1)]

group_pivot.columns = columns
# group_pivot.columns = group_pivot.columns.get_level_values(1)

group_pivot.head()

"""Each row is the history of each customer.<br/>
The time series are stored in wide-format - the series is stored in columns ranging from 1 to 1290, indicating the days since the start of the time period in question.
"""

group_pivot.shape

group_vars = pd.merge(group_pivot, customer_vars, left_index=True, right_on='phone_number', how='outer')

group_vars.n_rq.sum()

# solicitud_df.shape

group_vars.shape

group_vars.columns

group_vars.index = group_vars['phone_number']

"""#### Get a feel for some time series' shapes.
Before we jump into pre-processing, it's useful to look at a sample of series to see if anything jumps out at us that will motivate how to move forward.
"""

def series_from_id(df, _id:str) -> pd.DataFrame:
    """
    Get a daily time series for a single id
    """
    return df.loc[df.phone_number == _id]\
    .iloc[:, 0:1290]\
    .T

# Create a global lookup table for fast plotting by type phone.
daily_requests_typep_lookup = group_vars[['check_phone'] + list(group_vars.columns[0:1290])]\
    .melt(id_vars = 'check_phone')\
    .groupby('check_phone variable'.split())\
    .agg({'value':'sum'})


# Create a global lookup table for fast plotting by flance once.
daily_requests_flag_lookup = group_vars[['flag_once_rq'] + list(group_vars.columns[0:1290])]\
    .melt(id_vars = 'flag_once_rq')\
    .groupby('flag_once_rq variable'.split())\
    .agg({'value':'sum'})


def series_from_typephone(type_p:str) -> pd.DataFrame:
    return daily_requests_typep_lookup.loc[type_p]

def series_from_flagonce(flag:str) -> pd.DataFrame:
    return daily_requests_flag_lookup.loc[flag]

"""
Time series for particular items are quite noisy on a daily level.
Provide the ability to bin sales (for examply - to a weekly bin) for more stable plots
"""
def series_from_id_binned(df, _id:str, label, bin_every:int = 7) -> pd.DataFrame:
    """
    Get the sales for an id, grouped by a fixed interval (default 7 - weekly)
    """
    t = series_from_id(df, _id).reset_index()
    t['index'] = t.index.map(lambda x: x - (x % bin_every))
    t.columns = pd.Index([label, 'requests'])
    return t.groupby(label)\
        .agg({'requests':'sum'})


def series_from_typephone_binned(type_p:str, bin_every:int = 7) -> pd.DataFrame:
    """
    Get the sales for a department, grouped by a fixed interval (default 7 - weekly)
    """
    t = series_from_typephone(type_p).reset_index()
    t['variable'] = t.index.map(lambda x: x - (x % bin_every))
    return t.groupby('variable')\
        .agg({'value':'sum'})

def series_from_flagonce_binned(flag:str, bin_every:int = 7) -> pd.DataFrame:
    """
    Get the sales for a department, grouped by a fixed interval (default 7 - weekly)
    """
    t = series_from_flagonce(flag).reset_index()
    t['variable'] = t.index.map(lambda x: x - (x % bin_every))
    return t.groupby('variable')\
        .agg({'value':'sum'})

"""#### Plotting 5 unique customers (phone_number)"""

fig, axes = plt.subplots(nrows = 5, figsize = (12,20))
_ids = group_vars.phone_number.sample(n = 5, random_state = 1)
for i in range(len(_ids)):
    series_from_id(group_vars, _ids.iloc[i]).plot(ax = axes[i])
del _ids

"""We can see that there are customers with only once use of service per day, others use the service two times some days, and others use the service only once time and churn the service of the company.<br/>
We will bin requests by week, which will lead the number of requests per time unit to be less volatile.
"""

# bin the items by week and plot again
fig, axes = plt.subplots(nrows = 5, figsize = (12,20))
_ids = group_vars.phone_number.sample(n = 5, random_state = 1)
for i in range(len(_ids)):
    series_from_id_binned(group_vars, _ids.iloc[i], 'week', bin_every = 7).plot(ax = axes[i])

"""For all series, requests only start at a certain point. This is important, as if we are to train a model over the entire time period, we will dilute any real signal with zero requests. Instead, we need a way to clip the beginning of the series with no requests and only start training after the point that we have data.

Finally, plotting by type phone.
"""

fig, axes = plt.subplots(nrows = 4, figsize = (12,20))
random.seed(3)
_ids = ['other', 'len_4', 'landline', 'mobile']
for i in range(len(_ids)):
    series_from_typephone_binned(_ids[i], bin_every = 7).plot(ax = axes[i])
    axes[i].set_title('Type phone: %s' % _ids[i])

fig, axes = plt.subplots(nrows = 2, figsize = (12,20))
random.seed(3)
_ids = [0, 1]
for i in range(len(_ids)):
    series_from_flagonce_binned(_ids[i], bin_every = 7).plot(ax = axes[i])
    axes[i].set_title('Does the customer has requested a taxicab only once?: %s' % _ids[i])

pd.crosstab(group_vars.flag_once_rq, group_vars.check_phone).apply(lambda r: r/r.sum(), axis=1)

"""---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Un grupo son los clientes que solo tienen una solicitud, se haran clusters para los que tienen mas de una solicitud.
"""

series_not1_rq = group_vars[(group_vars.flag_once_rq == 0) & (group_vars[1] >= 0)]

series_not1_rq.shape

# create a global lookup table for fast plotting by phone
daily_requests_lookup = series_not1_rq[['phone_number'] + list(series_not1_rq.columns[0:1290])]\
    .melt(id_vars = 'phone_number')\
    .groupby('phone_number variable'.split())\
    .agg({'value':'sum'})

daily_requests_lookup.tail()

# Create a lookup table for scaled series
daily_requests_lookup_scaled = daily_requests_lookup\
    .pivot_table(index = 'variable', columns = 'phone_number', values = 'value').copy()

daily_requests_lookup_scaled = daily_requests_lookup_scaled.div(daily_requests_lookup_scaled.mean(axis = 0), axis = 1)

# bin by week
daily_requests_lookup_scaled_weekly = daily_requests_lookup_scaled.copy().reset_index()
daily_requests_lookup_scaled_weekly['variable'] = daily_requests_lookup_scaled_weekly.variable.map(lambda x: x - (x%7))
daily_requests_lookup_scaled_weekly = daily_requests_lookup_scaled_weekly.groupby('variable').mean()

daily_requests_lookup_scaled_weekly.head()

daily_requests_lookup_scaled_weekly.columns

## Align and plot with the Rabiner-Juang type VI-c unsmoothed recursion
dtw(
    daily_requests_lookup_scaled_weekly[5.0],\
    daily_requests_lookup_scaled_weekly[19.0],\
    keep_internals=True,
    step_pattern=rabinerJuangStepPattern(3, "c"))\
    .plot(type="twoway",offset=10);

group_vars[group_vars.mean_rq_q >= 2].head().index

def get_dtw_diff_matrix(cols:list):
    """
    From a list of series, compute a distance matrix by computing the
    DTW distance of all pairwise combinations of series.
    """
    diff_matrix = {}
    cross = itertools.product(cols, cols)
    for (col1, col2) in cross:
        series1 = daily_requests_lookup_scaled_weekly[col1]
        series2 = daily_requests_lookup_scaled_weekly[col2]
        diff = dtw(
            series1,
            series2,
            keep_internals=True,
            step_pattern=rabinerJuangStepPattern(2, "c")
            )\
            .normalizedDistance
        diff_matrix[(col1, col2)] = [diff]
    return diff_matrix

import itertools

# sample 50 series, and compute the DTW distance matrix
random.seed(1)
sample_cols = random.sample(list(daily_requests_lookup_scaled_weekly.columns), 50)
dtw_diff_dict = get_dtw_diff_matrix(sample_cols)
# make into a df
dtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\
    .rename(columns = {"level_0":"phone1", "level_1":"phone2", 0:"diff"})\
    .pivot_table(index = "phone1", columns = "phone2", values = "diff")

# plot a similarity matrix, with a dendogram imposed
import seaborn as sns
sns.clustermap(1-dtw_diff_df)

from scipy.cluster.hierarchy import fcluster, ward, dendrogram
# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean
w = ward(dtw_diff_df)
# extract clusters
dtw_clusters = pd.DataFrame({"cluster":fcluster(w, 3, criterion='maxclust')}, index = dtw_diff_df.index)

dtw_clusters.cluster.value_counts().sort_index().plot.barh()
plt.title("Frequency of DTW clusters", fontsize = 14)

"""### Showing cluster 1"""

# cluster 1
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 1],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

# cluster 1
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 1],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""### Showing cluster 2"""

# cluster 2
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 2],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

# cluster 2
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 2],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluster 3"""

# cluster 3
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 3],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

# cluster 3
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 3],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluster 4"""

# cluster 4
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 4],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluester 5"""

# cluster 5
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 5],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluster 6"""

# cluster 6
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 6],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluster 7"""

# cluster 7
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 7],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

"""#### Showing cluster 8"""

# cluster 8
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 8],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

# sample 200 series, and compute the DTW distance matrix
random.seed(1)
sample_cols = random.sample(list(daily_requests_lookup_scaled_weekly.columns), 200)
dtw_diff_dict = get_dtw_diff_matrix(sample_cols)
# make into a df
dtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\
    .rename(columns = {"level_0":"phone1", "level_1":"phone2", 0:"diff"})\
    .pivot_table(index = "phone1", columns = "phone2", values = "diff")

# plot a similarity matrix, with a dendogram imposed
import seaborn as sns
sns.clustermap(1-dtw_diff_df)

# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean
t = ward(dtw_diff_df)
# extract clusters
dtw_clusters = pd.DataFrame({"cluster":fcluster(t, 1.15)}, index = dtw_diff_df.index)

dtw_clusters.cluster.value_counts().sort_index().plot.barh()
plt.title("Frequency of DTW clusters", fontsize = 14)

"""#### Showing Cluster 7"""

# cluster 7
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 7],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

def plot_dtw(series1:str, series2:str) -> None:
    dtw(daily_requests_lookup_scaled_weekly[series1],\
            daily_requests_lookup_scaled_weekly[series2],\
        keep_internals=True,
        step_pattern=rabinerJuangStepPattern(2, "c"))\
        .plot(type="twoway",offset=5)

plot_dtw(7224884.0, 7327449.0)
plot_dtw(3117647300.0, 3165142570.0)
plot_dtw(3186070141.0, 3232884438.0)

"""#### Showing Cluster 6"""

# cluster 6
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 6],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

plot_dtw(7223134.0, 3107003150.0)
plot_dtw(7302565.0, 3143291074.0)
plot_dtw(3203489895.0, 3217495798.0)

"""#### Showing Cluster 15"""

# cluster 15
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 15],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

plot_dtw(7221004.0, 3104193389.0)
plot_dtw(3207881205.0, 3216256046.0)
plot_dtw(3015528777.0, 3046718392.0)

# cluster 15
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 11],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

plot_dtw(7207620.0, 3145509087.0)

# sample 200 series, and compute the DTW distance matrix
dtw_diff_dict = get_dtw_diff_matrix(daily_requests_lookup_scaled_weekly.columns)
# make into a df
dtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\
    .rename(columns = {"level_0":"phone1", "level_1":"phone2", 0:"diff"})\
    .pivot_table(index = "phone1", columns = "phone2", values = "diff")

# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean
t = ward(dtw_diff_df)
# extract clusters
# dtw_clusters = pd.DataFrame({"cluster":fcluster(t, 1.15)}, index = dtw_diff_df.index)

# extract clusters
dtw_clusters = pd.DataFrame({"cluster":fcluster(w, 3, criterion='maxclust')}, index = dtw_diff_df.index)

test = dtw_clusters.loc[dtw_clusters.cluster == 1]

# cluster 15
daily_requests_lookup_scaled_weekly.T.merge(
    dtw_clusters.loc[dtw_clusters.cluster == 1],
    left_index = True,
    right_index = True
)\
    .T\
    .plot(figsize = (12,4));

dtw_diff_dict.to_csv('dtw.csv')

"""---------------------------------------------------------------------------------------------------------------------------------------------------------------

<a href='#top'><span class="label label-info">Go to the table of content</span></a>
#### <a id='prop'>Time Series Forecasting With Prophet </a>

### Load and Plot Dataset
"""

sol_dir = pd.read_csv('Data/sol_dir.csv')

def introduce_nulls(df):
    df['date'] = pd.to_datetime(df.index)
    idx = pd.date_range(df.date.dt.date.min(), df.date.dt.date.max())
    df = df.set_index('date')
    df = df.reindex(idx)
    df.reset_index(inplace=True)
    df.rename(columns={'index':'date'},inplace=True)
    return df

def plot_metric(df, grp_var, var_intg, metric="requests' number"):
    df_grp = pd.DataFrame(df.groupby(grp_var)[var_intg].count().rename(metric))
    df_not_nulls = introduce_nulls(df_grp)
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_not_nulls.date, y=df_not_nulls[metric],
                            name='Total '+metric))

#   fig.update_yaxes(title_text='Total items sold' if metric=='sold' else 'Total revenue($)')
    fig.update_yaxes(title_text=metric)
    fig.update_layout(template='seaborn',title='Taxicabs Requests')
    fig.update_layout(
        xaxis=dict(
        #autorange=True,
        range = ['2017-01-01','2020-08-23'],
        rangeselector=dict(
            buttons=list([
                dict(count=1,
                     label="1day",
                     step="day",
                     stepmode="backward"),
                dict(count=1,
                     label="1m",
                     step="month",
                     stepmode="backward"),
                dict(count=6,
                     label="6m",
                     step="month",
                     stepmode="backward"),
                dict(count=1,
                     label="YTD",
                     step="year",
                     stepmode="todate"),
                dict(count=1,
                     label="1y",
                     step="year",
                     stepmode="backward"),
                dict(count=2,
                     label="2y",
                     step="year",
                     stepmode="backward"),
                dict(step="all")
            ])
        ),
        rangeslider=dict(
            autorange=True,
        ),
        type="date"
    ))
    return fig

plot_metric(sol_dir, 'date_x', 'id_x', metric="requests' number")

"""We can see that the requests' number is approximately 2.000 daily before the pandemic. There are very few requests in the covid lockdown period, and post covid, the taxicab requests have been recovering. The last point (August 19, 2020) seems to do not have the data ultimately."""

from pandas import to_datetime

# prepare data and expected column names
data = pd.DataFrame(sol_dir.groupby('date_x', as_index=False)['id_x'].count())
data['date_x'] = to_datetime(data.date_x)

"""#### Make an In-Sample and Out-of-Sample data frames.

We will cut off the data after Jun 2020 to use it as our validation set. We will train on the earlier data.
"""

split_date = '15-Jul-2020'
outlier_date = '19-Aug-2020'
data_inSample = data.loc[data.date_x <= split_date].copy()
data_outSample = data.loc[(data.date_x > split_date) & (data.date_x < outlier_date)].copy()

data_inSample.index = data_inSample.date_x
data_outSample.index = data_outSample.date_x

# Plot In-Sample and Out-of-Sample so you can see where we split

data_outSample[['id_x']].rename(columns={'id_x': 'Out-of-Sample'}).join(data_inSample[['id_x']].rename(columns={'id_x': 'In-Sample'}),
                                                                        how='outer') \
    .plot(figsize=(15,5), title='Taxicabs Requests')
plt.xlabel('Date')
plt.ylabel('Taxicabs Requests')
plt.show()

"""#### Simple Prophet Model"""

# Format data for prophet model using ds and y
data_inSample2 = data_inSample[['id_x']].reset_index().rename(columns={'date_x':'ds', 'id_x':'y'})

# define the model
model = fbprophet.Prophet()

# fit the model
model.fit(data_inSample2)

"""Prophet automatically detected daily data and disabled daily seasonality.<br/>"""

future_requests=model.make_future_dataframe(periods=210, freq='D')
forecast_rq=model.predict(future_requests)

"""We can plot the forecast by Prophet."""

model.plot(forecast_rq, xlabel='Time', ylabel='Taxicabs Requests')
plt.title('Taxicab requests Prediction');

forecast_rq.tail()

"""#### Trend Change Points
The trend in a real time series can change abruptly. Prophet attempts to detect these changes automatically using a Laplacian or double exponential prior. By default, the change points are only fitted for the 1st 80% of the time series, allowing sufficient runway for the actual forecast. In the taxicab requests data, note that there is a sharp dip in April 2020 due to the COVID outbreak. These outliers should ideally be removed. Let’s display the change points detected by Prophet:
"""

from fbprophet.plot import add_changepoints_to_plot
fig_req=model.plot(forecast_rq)
a=add_changepoints_to_plot(fig_req.gca(),model,forecast_rq)

"""Visually it appears that the general trend is correct but it is being underfit. For example the decline during the 2008-2009 financial crisis is not detected. To adjust the trend change, we can use the parameter changepoint_prior_scale which is set to 0.05 by default. Increasing its value would make the trend more flexible and reduce underfitting, at the risk of overfitting. Let us set it to 0.5 as suggested by the Prophet Documentation Guide. If we want to generate uncertainty intervals for the trend and seasonality components, we need to perform full Bayesian sampling, which can be done by using the mcmc_samples parameter in Prophet."""

model2 = fbprophet.Prophet(changepoint_prior_scale=0.5)
forecast_rq2 = model2.fit(data_inSample2).predict(future_requests)
fig = model2.plot(forecast_rq2)
a=add_changepoints_to_plot(fig.gca(),model2,forecast_rq2)

forecast_rq2.tail()

# Format data for prophet model using ds and y
data_outSample2 = data_outSample[['id_x']].reset_index().rename(columns={'date_x':'ds', 'id_x':'y'})

# Predict on training set with model
outS_fcst2 = model2.predict(data_outSample2)

# Plot the forecast
fig = model2.plot(outS_fcst2)
plt.show()

"""#### Compare Forecast to Actuals"""

# Plot the forecast with the actuals
f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(15)
ax.scatter(data_outSample.index, data_outSample['id_x'], color='r')
fig = model.plot(outS_fcst2, ax=ax)

"""### Error Metrics"""

from sklearn.metrics import mean_squared_error, mean_absolute_error

def mean_absolute_percentage_error(y_true, y_pred):
    """Calculates MAPE given y_true and y_pred"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


print(mean_squared_error(y_true=data_outSample2.y, y_pred=outS_fcst2.yhat))
print(mean_absolute_error(y_true=data_outSample2.y, y_pred=outS_fcst2.yhat))
print(mean_absolute_percentage_error(y_true=data_outSample2.y, y_pred=outS_fcst2.yhat))

"""#### Plot Model Components
We get a better fit of the trend when increasing changepoint_prior_scale. The decline during the financial crisis can be detected. The user can also manually define the change points based on domain knowledge (e.g. when forecasting sales the analyst might be aware of new product launches, sales, etc.) Using the plot_components function we can display the components of the model:
"""

model2.plot_components(outS_fcst2)

"""We observe a piecewise linear trend. Prophet also has the ability to fit saturating trends using a logistic growth trend model. This is applicable in cases where the trend is limited by capacity, e.g. the number of Facebook users in a country would be naturally limited by the number of people with access to the internet. This is done by setting the parameter growth=logistic and defining a column called cap in the dataframe.

#### Cross Validation
We can perform cross validation to measure forecast error. Cut off points are selected and we train the model with data up to that point. We can then compare the prediction vs actual data over a specified time horizon. This can be done using the cross_validation function. The parameter period specifies the interval between cut off points.
"""

from fbprophet.diagnostics import cross_validation
df_cv = cross_validation(model2, initial='1000 days',
                         period='90 days', horizon = '180 days')

from fbprophet.diagnostics import performance_metrics
from fbprophet.plot import plot_cross_validation_metric

df_p = performance_metrics(df_cv)
df_p.head()

"""#### Adding Holidays

Next we will see if adding holiday indicators will help the accuracy of the model. Prophet comes with a Holiday Effects parameter that can be provided to the model prior to training.
"""

holiday_df = pd.read_csv('Data/holidays.csv')

holiday_df.head()

holiday_df.rename(columns={'date':'ds', 'celebration':'holiday'}, inplace=True)

holiday_df['ds'] = pd.to_datetime(holiday_df['ds'])

holiday_df.head()

# Setup and train model with holidays
model_with_holidays = fbprophet.Prophet(changepoint_prior_scale=0.5, holidays=holiday_df)
model_with_holidays.fit(data_inSample2)

"""#### Predict With Holidays"""

# Predict on out sample with model
rq_os_fcst_with_hols = model_with_holidays.predict(df=data_outSample2)

"""### Plot Holiday Effect"""

fig2 = model_with_holidays.plot_components(rq_os_fcst_with_hols)

"""#### Error Metrics with Holidays Added"""

print(mean_squared_error(y_true=data_outSample2.y, y_pred=rq_os_fcst_with_hols.yhat))
print(mean_absolute_error(y_true=data_outSample2.y, y_pred=rq_os_fcst_with_hols.yhat))
print(mean_absolute_percentage_error(y_true=data_outSample2.y, y_pred=rq_os_fcst_with_hols.yhat))

forecast_rqHds = model_with_holidays.predict(future_requests)
fig = model_with_holidays.plot(forecast_rqHds)
a=add_changepoints_to_plot(fig.gca(),model_with_holidays,forecast_rqHds)

"""#### Compare Models Just for Holiday Dates

Lets plot Forecast model with and without holidays for 4th of July. It does look like the model with holidays is more accurate for this holiday.
"""

carnaval = forecast_rqHs[(forecast_rqHs.ds >= '2020-01-02') & (forecast_rqHs.ds <= '2020-01-07')]

carnaval_actual = data_inSample[(data_inSample.date_x >= '2020-01-02') & (data_inSample.date_x <= '2020-01-07')]

# Plot the forecast with the actuals
# f, ax = plt.subplots(1)
# f.set_figheight(5)
# f.set_figwidth(15)
# ax.scatter(carnaval_actual.index, carnaval_actual['id_x'], color='r')
# fig = model_with_holidays.plot(carnaval, ax=ax)
# ax.set_xbound(lower='2020-01-02', upper='2020-02-07')
# ax.set_ylim(0, 4000)
# plot = plt.suptitle('Week of July 4th Forecast vs Actuals non-Holiday Model')

forecast_rqHds.to_csv('Data/model_HDS.csv')

forecast_rqHds['yhat'] = forecast_rqHds['yhat'].astype('int')

figMHDS = plot_plotly(model_with_holidays, forecast_rqHds)  # This returns a plotly Figure
figMHDS.update_layout(template='seaborn', title='Taxicab Requests', xaxis_title='Date', yaxis_title='Requests')
py.iplot(figMHDS)

# Python
import json
from fbprophet.serialize import model_to_json, model_from_json

with open('serialized_model.json', 'w') as fout:
    json.dump(model_to_json(model_with_holidays), fout)  # Save model

with open('serialized_model.json', 'r') as fin:
    m = model_from_json(json.load(fin))  # Load model

figMHDS.write_html("Data/model_HDS.html")

from fbprophet.plot import plot_plotly
import plotly.offline as py
py.init_notebook_mode()

figM2 = plot_plotly(model2, forecast_rq2)  # This returns a plotly Figure
figM2.update_layout(template='seaborn', title='Taxicab Requests', xaxis_title='Date', yaxis_title='Requests')
py.iplot(figM2)

"""#### Predecir por horas"""

sol_dir['date_time'] = sol_dir.creado.str[:14] + '00:00'

sol_dir['date_time'] = to_datetime(sol_dir.date_time)

# prepare data and expected column names
dataH = pd.DataFrame(sol_dir.groupby('date_time', as_index=False)['id_x'].count())

data_inSampleH = dataH.loc[dataH.date_time <= split_date].copy()
data_outSampleH = dataH.loc[(dataH.date_time > split_date) & (dataH.date_time < outlier_date)].copy()

data_inSampleH.index = data_inSampleH.date_time
data_outSampleH.index = data_outSampleH.date_time

data_inSampleH2 = data_inSampleH[['id_x']].reset_index().rename(columns={'date_time':'ds', 'id_x':'y'})
data_outSampleH2 = data_outSampleH[['id_x']].reset_index().rename(columns={'date_time':'ds', 'id_x':'y'})

modelH = fbprophet.Prophet(changepoint_prior_scale=0.5)
modelH.fit(data_inSampleH2)
future_requestsH=modelH.make_future_dataframe(periods=5040, freq='H')

forecast_rqH = modelH.predict(future_requestsH)

fig_req=modelH.plot(forecast_rqH)
a=add_changepoints_to_plot(fig_req.gca(),modelH,forecast_rqH)

# Predict on Out sample set with model
outS_fcstH = modelH.predict(data_outSampleH2)

# Plot the forecast
fig = modelH.plot(outS_fcstH)
plt.show()

print(mean_squared_error(y_true=data_outSampleH2.y, y_pred=outS_fcstH.yhat))
print(mean_absolute_error(y_true=data_outSampleH2.y, y_pred=outS_fcstH.yhat))
print(mean_absolute_percentage_error(y_true=data_outSampleH2.y, y_pred=outS_fcstH.yhat))

fig = plot_plotly(modelH, forecast_rqH)  # This returns a plotly Figure
py.iplot(fig)

modelHs = fbprophet.Prophet(changepoint_prior_scale=0.5, holidays=holiday_df)
modelHs.fit(data_inSampleH2)
# future_requestsH=modelH.make_future_dataframe(periods=5040, freq='H')

with open('serialized_model_H.json', 'w') as fout:
    json.dump(model_to_json(modelHs), fout)  # Save model

forecast_rqHs = modelHs.predict(future_requestsH)

fig_req=modelHs.plot(forecast_rqHs)
a=add_changepoints_to_plot(fig_req.gca(),modelHs,forecast_rqHs)

# Predict on Out sample set with model
outS_fcstHs = modelHs.predict(data_outSampleH2)

# Plot the forecast
fig = modelHs.plot(outS_fcstHs)
plt.show()

print(mean_squared_error(y_true=data_outSampleH2.y, y_pred=outS_fcstHs.yhat))
print(mean_absolute_error(y_true=data_outSampleH2.y, y_pred=outS_fcstHs.yhat))
print(mean_absolute_percentage_error(y_true=data_outSampleH2.y, y_pred=outS_fcstHs.yhat))

forecast_rqHs.tail()

forecast_rqHs.to_csv('Data/model_HDS_H.csv')

forecast_rqHs['yhat'] = forecast_rqHs['yhat'].astype('int')

figMHDS_H = plot_plotly(modelHs, forecast_rqHs)  # This returns a plotly Figure
figMHDS_H.update_layout(template='seaborn', title='Taxicab Requests', xaxis_title='Date', yaxis_title='Requests')
py.iplot(figMHDS_H)

figMHDS_H.write_html("Data/model_HDS_H.html")

forecast_rqHs[forecast_rqHs.ds >= '2020-10-12'].head(24)

def introduce_nulls(df):
    idx = pd.date_range(df.fecha.dt.date.min(), df.fecha.dt.date.max())
    #df = df.set_index('fecha')
    df = df.reindex(idx)
    df.reset_index(inplace=True)
    df.rename(columns={'index': 'date'}, inplace=True)
    return df

def calmap(df, title, scale):
    years = df.year.unique().tolist()
    fig = make_subplots(rows=len(years), cols=1, shared_xaxes=True, vertical_spacing=0.005)
    r = 1
    for year in years:
        data = df[df['year'] == year]
        data = introduce_nulls(data)
        fig.add_trace(go.Heatmap(
            z=data.Count,
            x=data.week,
            y=data.weekday,
            hovertext=data.fecha.dt.date,
            coloraxis="coloraxis",
            name=year,),
            r, 1)
        fig.update_yaxes(title_text=year, tickfont=dict(size=5), row=r, col=1)
        r += 1
    fig.update_xaxes(range=[1, 53], tickfont=dict(size=10), nticks=53)
    fig.update_layout(coloraxis={'colorscale': scale})
    fig.update_layout(template='seaborn', title=title)
    return fig
calmap(no_celebr, 't', 'YlGnBu')